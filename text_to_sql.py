# -*- coding: utf-8 -*-
"""Project_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BkNEhPdfUC5wFtbmmuPlMI_vbM1Oh_L1
"""

# CELL 1: INSTALL PACKAGES
# ============================================================================

print("Installing packages...")

# FIX: Uninstall and reinstall numpy to avoid binary incompatibility
!pip uninstall -y numpy
!pip install numpy==1.24.3

# Install other packages (remove version pins that might conflict)
!pip install -q langchain==0.1.20 langchain-community langchain-huggingface
!pip install -q transformers accelerate bitsandbytes sentence-transformers
!pip install -q chromadb sqlalchemy faiss-cpu

print("‚úÖ Installation complete!")
print("üîÑ IMPORTANT: You MUST restart runtime now!")
print("   Go to: Runtime ‚Üí Restart runtime")
print("   Then run Cell 2 onwards\n")

# CELL 2: IMPORTS
# ============================================================================

import urllib.request
import sqlite3

# LangChain imports
from langchain_community.utilities import SQLDatabase
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.prompts import FewShotPromptTemplate, PromptTemplate
from langchain.chains import LLMChain
from langchain_huggingface import HuggingFacePipeline

# Transformers
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline

print("‚úÖ Imports successful!\n")

# CELL 3: DOWNLOAD DATABASE & CREATE LANGCHAIN SQLDATABASE
# ============================================================================

print("Setting up database...")

# Download Chinook
url = "https://github.com/lerocha/chinook-database/raw/master/ChinookDatabase/DataSources/Chinook_Sqlite.sqlite"
urllib.request.urlretrieve(url, "chinook.db")

# Create LangChain SQLDatabase (automatic schema extraction!)
db = SQLDatabase.from_uri("sqlite:///chinook.db")

print(f"‚úÖ Database ready!")
print(f"   Tables: {db.get_usable_table_names()}\n")

# CELL 4: CREATE VECTOR STORE WITH LANGCHAIN (RAG)
# ============================================================================

print("Creating vector store...")

# Get schema info using LangChain
table_info = db.get_table_info()

# Create documents (one per table)
from langchain.schema import Document

docs = []
for table in db.get_usable_table_names():
    # Get table schema
    conn = sqlite3.connect("chinook.db")
    cursor = conn.cursor()
    cursor.execute(f"PRAGMA table_info({table})")
    cols = cursor.fetchall()
    conn.close()

    # Create document
    text = f"Table: {table}\n"
    text += "Columns: " + ", ".join([f"{col[1]} ({col[2]})" for col in cols])

    docs.append(Document(page_content=text, metadata={"table": table}))

# Create FAISS vector store with LangChain
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
vectorstore = FAISS.from_documents(docs, embeddings)

# Create retriever
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

print(f"‚úÖ Vector store created with {len(docs)} documents!")

# Test retrieval
test_docs = retriever.get_relevant_documents("customers and invoices")
print(f"   Test: Found {len(test_docs)} relevant tables\n")

# CELL 5: CREATE LANGCHAIN PROMPT TEMPLATE
# ============================================================================

print("Creating LangChain prompt template...")

# Few-shot examples
examples = [
    {
        "question": "List all artists",
        "answer": "SELECT * FROM Artist LIMIT 10;"
    },
    {
        "question": "Top 5 customers by spending",
        "answer": """SELECT c.FirstName, c.LastName, SUM(i.Total) as Total
FROM Customer c
JOIN Invoice i ON c.CustomerId = i.CustomerId
GROUP BY c.CustomerId
ORDER BY Total DESC
LIMIT 5;"""
    },
    {
        "question": "Most popular genres",
        "answer": """SELECT g.Name, COUNT(t.TrackId) as Count
FROM Genre g
JOIN Track t ON g.GenreId = t.GenreId
GROUP BY g.GenreId
ORDER BY Count DESC;"""
    }
]

# Example template
example_template = """
Question: {question}
SQL: {answer}
"""

example_prompt = PromptTemplate(
    input_variables=["question", "answer"],
    template=example_template
)

# Main prompt
prefix = """You are a SQLite expert. Generate a syntactically correct SQLite query.

Here are examples:"""

suffix = """
Database Schema:
{schema}

Question: {question}
SQL:"""

# Create FewShotPromptTemplate (LangChain!)
few_shot_prompt = FewShotPromptTemplate(
    examples=examples,
    example_prompt=example_prompt,
    prefix=prefix,
    suffix=suffix,
    input_variables=["schema", "question"],
    example_separator="\n"
)

print("‚úÖ LangChain prompt template ready!\n")

# CELL 6: LOAD MODELS (5-10 MINUTES)
# ============================================================================

print("Loading models (takes 5-10 minutes)...\n")

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Device: {device}")

# Quantization
quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16
)

# --- Load SQLCoder ---
print("\nüì• Loading SQLCoder...")
try:
    sqlcoder_tokenizer = AutoTokenizer.from_pretrained("defog/sqlcoder-7b-2")
    sqlcoder_model = AutoModelForCausalLM.from_pretrained(
        "defog/sqlcoder-7b-2",
        quantization_config=quant_config,
        device_map="auto",
        trust_remote_code=True
    )

    # Create pipeline and wrap with LangChain
    sqlcoder_pipe = pipeline(
        "text-generation",
        model=sqlcoder_model,
        tokenizer=sqlcoder_tokenizer,
        max_new_tokens=300,
        temperature=0.1
    )
    sqlcoder_llm = HuggingFacePipeline(pipeline=sqlcoder_pipe)

    print("‚úÖ SQLCoder loaded & wrapped in LangChain!")
except Exception as e:
    print(f"‚ùå SQLCoder failed")
    sqlcoder_llm = None

# --- Load Llama/CodeLlama ---
print("\nüì• Loading Llama...")
llama_models = ["codellama/CodeLlama-7b-Instruct-hf", "mistralai/Mistral-7B-Instruct-v0.2"]
llama_llm = None
llama_name = None

for model_name in llama_models:
    try:
        print(f"   Trying {model_name}...")
        llama_tokenizer = AutoTokenizer.from_pretrained(model_name)
        llama_model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quant_config,
            device_map="auto"
        )

        # Create pipeline and wrap with LangChain
        llama_pipe = pipeline(
            "text-generation",
            model=llama_model,
            tokenizer=llama_tokenizer,
            max_new_tokens=300,
            temperature=0.1
        )
        llama_llm = HuggingFacePipeline(pipeline=llama_pipe)
        llama_name = model_name.split('/')[-1]

        print(f"‚úÖ {llama_name} loaded & wrapped in LangChain!")
        break
    except:
        continue

print("\n‚úÖ Models ready!\n")

# CELL 7: CREATE LANGCHAIN CHAINS
# ============================================================================

print("Creating LangChain chains...")

# Create chains (LangChain handles everything!)
sqlcoder_chain = LLMChain(
    llm=sqlcoder_llm,
    prompt=few_shot_prompt
) if sqlcoder_llm else None

llama_chain = LLMChain(
    llm=llama_llm,
    prompt=few_shot_prompt
) if llama_llm else None

print("‚úÖ LangChain chains created!\n")

# CELL 8: MAIN FUNCTION USING LANGCHAIN
# ============================================================================

def ask_question(question):
    """
    Main function using LangChain components:
    - Retriever (RAG)
    - Chains (LLM)
    - SQLDatabase (execution)
    """
    print(f"\n{'='*60}")
    print(f"üìù Question: {question}")
    print('='*60)

    # Step 1: Retrieve schema using LangChain retriever
    relevant_docs = retriever.get_relevant_documents(question)
    schema = "\n".join([doc.page_content for doc in relevant_docs])

    # Step 2: Generate SQL with SQLCoder using LangChain chain
    if sqlcoder_chain:
        print("\nü§ñ SQLCoder (via LangChain):")
        try:
            response = sqlcoder_chain.run(schema=schema, question=question)

            # Extract SQL
            if "SQL:" in response:
                sql = response.split("SQL:")[-1].strip()
            else:
                sql = response.strip()
            sql = sql.split('\n\n')[0].strip()

            print(f"   SQL: {sql}")

            # Execute using LangChain SQLDatabase
            try:
                result = db.run(sql)
                print(f"   ‚úÖ Success! Result: {str(result)[:100]}")
            except Exception as e:
                print(f"   ‚ùå Error: {str(e)[:100]}")
        except Exception as e:
            print(f"   ‚ùå Generation failed: {str(e)[:100]}")

    # Step 3: Generate SQL with Llama using LangChain chain
    if llama_chain:
        print(f"\nü§ñ {llama_name} (via LangChain):")
        try:
            response = llama_chain.run(schema=schema, question=question)

            # Extract SQL
            if "SQL:" in response:
                sql = response.split("SQL:")[-1].strip()
            else:
                sql = response.strip()
            sql = sql.split('\n\n')[0].strip()

            print(f"   SQL: {sql}")

            # Execute using LangChain SQLDatabase
            try:
                result = db.run(sql)
                print(f"   ‚úÖ Success! Result: {str(result)[:100]}")
            except Exception as e:
                print(f"   ‚ùå Error: {str(e)[:100]}")
        except Exception as e:
            print(f"   ‚ùå Generation failed: {str(e)[:100]}")

print("‚úÖ Main function ready (using LangChain)!\n")

# CELL 9: TEST QUERIES
# ============================================================================

print("="*60)
print("TESTING MODELS")
print("="*60)

test_questions = [
    "List the top 5 artists with most albums",
    "Show me all customers from Canada",
    "What is the total revenue?",
    "Find the longest track",
    "Which employee has the most customers?"
]

for q in test_questions:
    ask_question(q)

print("\n" + "="*60)
print("üéâ PROJECT COMPLETE!")
print("="*60)
print("\n‚úÖ This project uses LangChain:")
print("   - FewShotPromptTemplate for prompts")
print("   - HuggingFacePipeline for model wrapping")
print("   - LLMChain for orchestration")
print("   - FAISS for vector store")
print("   - SQLDatabase for DB operations")
print("\nTry your own:")
print("ask_question('Show me all rock albums')")
print("ask_question('Which genre has most tracks?')")

#CELL 10: Enhanced Error Handling & Query Validation
import re

def extract_sql_from_response(response):
    """Extract only the SQL query from model response"""
    # Try to find SQL after the last "SQL:" marker
    if "SQL:" in response:
        parts = response.split("SQL:")
        sql = parts[-1].strip()

        # Remove everything after the semicolon
        if ';' in sql:
            sql = sql.split(';')[0] + ';'

        # Remove any trailing text after newlines
        sql = sql.split('\n')[0].strip()

        return sql

    return response.strip()

def validate_sql_query(sql_query):
    """Validate SQL query for safety"""
    dangerous_keywords = ['DROP', 'DELETE', 'TRUNCATE', 'ALTER', 'UPDATE']
    sql_upper = sql_query.upper()

    for keyword in dangerous_keywords:
        if keyword in sql_upper:
            return False, f"Dangerous keyword detected: {keyword}"

    if not sql_query.strip().endswith(';'):
        sql_query += ';'

    return True, sql_query

def ask_question_enhanced(question, model_name="SQLCoder"):
    """Production-ready version with error handling"""
    print(f"\n{'='*60}")
    print(f"üìù Question: {question}")
    print(f"{'='*60}\n")

    try:
        # Get schema
        schema = db.get_table_info()

        # Generate SQL
        if model_name == "SQLCoder":
            raw_response = sqlcoder_chain.run(question=question, schema=schema)
        else:
            raw_response = llama_chain.run(question=question, schema=schema)

        # Extract only the SQL query from response
        sql_query = extract_sql_from_response(raw_response)

        # Validate query
        is_valid, result = validate_sql_query(sql_query)
        if not is_valid:
            print(f"‚ùå Validation Failed: {result}")
            return None

        sql_query = result
        print(f"ü§ñ {model_name}:")
        print(f"   SQL: {sql_query}")

        # Execute query
        query_result = db.run(sql_query)
        print(f"   ‚úÖ Success! Result: {query_result}")

        return {
            "success": True,
            "question": question,
            "sql": sql_query,
            "result": query_result
        }

    except Exception as e:
        print(f"   ‚ùå Error: {str(e)}")
        return {
            "success": False,
            "question": question,
            "sql": sql_query if 'sql_query' in locals() else "N/A",
            "error": str(e)
        }

#CELL 11: Comprehensive Test Suite
# Expanded test cases for thorough evaluation
comprehensive_tests = [
    # Basic queries
    "Show me all customers from Canada",
    "What is the total revenue?",
    "Find the longest track",

    # Aggregations
    "How many tracks are there in total?",
    "What is the average invoice total?",

    # JOINs
    "Which employee has the most customers?",
    "List top 5 artists with most albums",
    "Show all albums by AC/DC",

    # WHERE clauses with operators
    "Find all invoices greater than $10",
    "Show tracks longer than 5 minutes",

    # Complex business queries
    "Which genre generates the most revenue?",
    "What are the top 3 countries by number of customers?",
    "Find the customer who spent the most money",

    # Edge cases
    "List tracks with no composer",
    "Show invoices from 2009"
]

print("="*60)
print("üß™ RUNNING COMPREHENSIVE TEST SUITE")
print("="*60)

# Test both models
test_results = {
    "SQLCoder": [],
    "CodeLlama": []
}

for question in comprehensive_tests:
    # Test SQLCoder
    result_sqlcoder = ask_question_enhanced(question, "SQLCoder")
    test_results["SQLCoder"].append(result_sqlcoder)

    # Test CodeLlama
    result_codellama = ask_question_enhanced(question, "CodeLlama")
    test_results["CodeLlama"].append(result_codellama)

print("\n" + "="*60)
print("üìä TEST RESULTS SUMMARY")
print("="*60)

for model_name, results in test_results.items():
    passed = sum(1 for r in results if r and r.get('success'))
    total = len(results)
    percentage = (passed/total)*100 if total > 0 else 0

    print(f"\n{model_name}:")
    print(f"   ‚úÖ Passed: {passed}/{total} ({percentage:.1f}%)")
    print(f"   ‚ùå Failed: {total-passed}/{total}")

print("\n" + "="*60)

#CELL 12: Performance Metrics & Model Comparison
import time
from datetime import datetime

def benchmark_models(test_questions):
    """Compare models with performance metrics"""
    results = {
        "SQLCoder": {"success": 0, "failed": 0, "total_time": 0},
        "CodeLlama": {"success": 0, "failed": 0, "total_time": 0}
    }

    print("\n" + "="*60)
    print("‚ö° PERFORMANCE BENCHMARK")
    print("="*60)

    for model in ["SQLCoder", "CodeLlama"]:
        print(f"\nüî¨ Testing {model}...\n")

        for question in test_questions:
            start_time = time.time()
            result = ask_question_enhanced(question, model)
            execution_time = time.time() - start_time

            results[model]["total_time"] += execution_time

            if result and result.get('success'):
                results[model]["success"] += 1
            else:
                results[model]["failed"] += 1

    # Display comparison
    print("\n" + "="*60)
    print("üèÜ MODEL COMPARISON DASHBOARD")
    print("="*60)

    for model, stats in results.items():
        total = stats["success"] + stats["failed"]
        success_rate = (stats["success"]/total)*100 if total > 0 else 0
        avg_time = stats["total_time"]/total if total > 0 else 0

        print(f"\n{model}:")
        print(f"   Success Rate: {success_rate:.1f}% ({stats['success']}/{total})")
        print(f"   Avg Time: {avg_time:.2f}s")
        print(f"   Total Time: {stats['total_time']:.2f}s")

    # Determine winner
    sqlcoder_rate = (results["SQLCoder"]["success"]/(results["SQLCoder"]["success"]+results["SQLCoder"]["failed"]))*100
    codellama_rate = (results["CodeLlama"]["success"]/(results["CodeLlama"]["success"]+results["CodeLlama"]["failed"]))*100

    winner = "SQLCoder" if sqlcoder_rate > codellama_rate else "CodeLlama"
    print(f"\nü•á Winner: {winner}")
    print("="*60)

# Run benchmark on subset of questions
benchmark_questions = [
    "What is the total revenue?",
    "Show me all customers from Canada",
    "Which employee has the most customers?",
    "Find the longest track",
    "List top 5 artists with most albums"
]

benchmark_models(benchmark_questions)

!pip install -U gradio langchain langchain-community langchain-huggingface transformers accelerate bitsandbytes chromadb sqlalchemy huggingface_hub

import sqlite3
import pandas as pd
import gradio as gr

def ask_question(user_db, question):
    # connect to uploaded DB
    conn = sqlite3.connect(user_db.name)
    cursor = conn.cursor()

    # Generate SQL
    prompt = f"Translate this question into an SQLite query:\n{question}\nSQL:"
    sql_query = sqlcoder_llm(prompt)

    try:
        cursor.execute(sql_query)
        rows = cursor.fetchall()
        columns = [desc[0] for desc in cursor.description]
        df = pd.DataFrame(rows, columns=columns)
        conn.close()
        return sql_query, df
    except Exception as e:
        conn.close()
        return f"‚ùå Error executing query: {e}", None

demo = gr.Interface(
    fn=ask_question,
    inputs=[
        gr.File(label="Upload SQLite Database (.db)"),
        gr.Textbox(label="Ask your question")
    ],
    outputs=[
        gr.Textbox(label="Generated SQL Query"),
        gr.Dataframe(label="Query Result")
    ],
    title="üß† Text-to-SQL on Your Own Database",
    description="Upload your SQLite database and ask natural language questions."
)

demo.launch()

# Commented out IPython magic to ensure Python compatibility.
# %%writefile requirements.txt
# gradio
# langchain
# langchain-community
# langchain-huggingface
# transformers
# accelerate
# bitsandbytes
# chromadb
# sqlalchemy
# huggingface_hub
# pandas
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile README.md
# # üß† Text-to-SQL Chatbot (with SQLCoder)
# 
# This app lets you **ask natural language questions** about your database ‚Äî and get results instantly.
# Just **upload any `.db` or `.sqlite` file**, and the model will:
# 
# 1. Convert your question to an SQL query
# 2. Run it on the uploaded database
# 3. Display both the generated SQL and output table
# 
# ### üöÄ Model
# Powered by **SQLCoder** (Open Source LLM fine-tuned for Text-to-SQL).
# 
# ### üõ†Ô∏è Tech Stack
# - LangChain
# - Hugging Face Transformers
# - Gradio
# - SQLite + Pandas
# 
# ### üí° Example
# **Question:** "Show all invoices from 2009"
# **Output:** SQL query + Table of results
# 
# ---
# 
# üë®‚Äçüíª Built with ‚ù§Ô∏è using Gradio and LangChain.
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import sqlite3
# import pandas as pd
# import gradio as gr
# from langchain_community.llms import HuggingFacePipeline
# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
# 
# # ============================================================
# # üöÄ Load SQLCoder model
# # ============================================================
# model_id = "defog/sqlcoder-7b-2"
# 
# tokenizer = AutoTokenizer.from_pretrained(model_id)
# model = AutoModelForCausalLM.from_pretrained(
#     model_id,
#     torch_dtype="auto",
#     device_map="auto"
# )
# 
# pipe = pipeline(
#     "text-generation",
#     model=model,
#     tokenizer=tokenizer,
#     max_new_tokens=256,
#     do_sample=False
# )
# 
# sqlcoder_llm = HuggingFacePipeline(pipeline=pipe)
# 
# # ============================================================
# # üß† Define query function
# # ============================================================
# def ask_question(user_db, question):
#     """Takes an uploaded SQLite database + a question, returns SQL + result"""
#     if not user_db:
#         return "‚ùå Please upload a database file.", None
# 
#     conn = sqlite3.connect(user_db.name)
#     cursor = conn.cursor()
# 
#     # Create a Text-to-SQL prompt
#     prompt = f"Translate this question into an SQLite query:\nQuestion: {question}\nSQL:"
#     sql_query = sqlcoder_llm(prompt)
# 
#     try:
#         cursor.execute(sql_query)
#         rows = cursor.fetchall()
#         columns = [desc[0] for desc in cursor.description]
#         df = pd.DataFrame(rows, columns=columns)
#         conn.close()
#         return sql_query, df
#     except Exception as e:
#         conn.close()
#         return f"‚ùå Error executing query: {e}", None
# 
# # ============================================================
# # üé® Gradio UI
# # ============================================================
# demo = gr.Interface(
#     fn=ask_question,
#     inputs=[
#         gr.File(label="Upload SQLite Database (.db)"),
#         gr.Textbox(label="Ask your question")
#     ],
#     outputs=[
#         gr.Textbox(label="Generated SQL Query"),
#         gr.Dataframe(label="Query Result")
#     ],
#     title="üß† Text-to-SQL on Your Own Database",
#     description="Upload your SQLite database and ask natural language questions."
# )
# 
# if __name__ == "__main__":
#     demo.launch()
#

!wget https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sqlite -O Chinook.db

